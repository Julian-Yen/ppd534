{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3wkIQ9as-hF"
   },
   "source": [
    "# PART A: Basic Content Analysis with Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fgbu2Lmz-v9y"
   },
   "source": [
    "Scraping qualitative data from the web is less intimidating than it sounds, but you will need the right tools to begin your analysis. In this section, we will run through the basics of Tweet scraping using the Twitter API Search Endpoint and discuss how to tailor your results to your specific needs. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Please note you will be unable to execute commands from the first section. An output file (ca_prop_tweets.csv) is provided in the datafolder for use in section 2.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "There are a few reasons why you are unable to follow along in this first section. First, our class Python environment is missing several key packages needed to interact with Twitter and make meaning from tweets. These packages include tweepy (one of the more widely used packages to interact with Twitter's API endpoints) and textblob (a natural language processing package with several useful text analysis methods). We could easily install these to the class environment, but this would require that we take a few unnecessary risks. Second, to interact with Twitter via their API, you will need consumer and access tokens so that Twitter can monitor your search/stream requests. We do not have time to create Twitter developer accounts today, unfortunately, but you can find more information on this using the link below.\n",
    "\n",
    "<br>\n",
    "\n",
    "https://developer.twitter.com/en\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Other useful links:\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "*   Twitter Search API parameters: https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets\n",
    "*   Tweet object data dictionary: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/tweet-object\n",
    "*   tweepy reference for interacting with Twitter API: http://docs.tweepy.org/en/v3.5.0/api.html#tweepy-api-twitter-api-wrapper\n",
    "*   textblob reference for NPL processes: https://textblob.readthedocs.io/en/dev/api_reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SKzZDa7i8aQ"
   },
   "source": [
    "## 1: A brief introduction to web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXxPkLe_iyel"
   },
   "outputs": [],
   "source": [
    "from tweepy import API \n",
    "from tweepy import Cursor\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qex2JJ0ljgG9"
   },
   "outputs": [],
   "source": [
    "consumer_key = \"INSERT YOUR CONSUMER KEY HERE\"\n",
    "consumer_secret = \"INSERT YOUR CONSUMER KEY SECRET HERE\"\n",
    "access_token = \"INSERT YOUR ACCESS TOKEN HERE\"\n",
    "access_secret = \"INSERT YOUR ACCESS TOKEN SECRET HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeEJ0xkSjtc9"
   },
   "outputs": [],
   "source": [
    "# in order to interact with any Twitter API endpoint, you will need to prove your identity...\n",
    "# the tweepy package initiates this process via the OAuthHandler class\n",
    "\n",
    "class TwitterAuthenticator():\n",
    "\n",
    "    def authenticate_twitter_app(self):\n",
    "        auth = OAuthHandler(consumer_key, \n",
    "                            consumer_secret)\n",
    "        auth.set_access_token(access_token, \n",
    "                              access_secret)\n",
    "        return auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmtt19C-j3KN"
   },
   "source": [
    "### 1.1: Accessing the Twitter Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJHqM3TYj4lF"
   },
   "outputs": [],
   "source": [
    "# the two primary tasks you are likely to use with tweepy are (a) streaming live tweets and (b) searching for existing tweets\n",
    "# the primary function of this class is to interact with the SEARCH endpoint \n",
    "\n",
    "class TwitterClient():\n",
    "\n",
    "    # __init__() is a built-in function for every class that executes whenever the function is called\n",
    "    # here, we are saying hello to the Twitter API\n",
    "\n",
    "    def __init__(self, twitter_user=None):\n",
    "        self.auth = TwitterAuthenticator().authenticate_twitter_app()\n",
    "        self.twitter_client = API(self.auth)\n",
    "\n",
    "        self.twitter_user = twitter_user\n",
    "\n",
    "    # here, we are requesting access to tweets from the Twitter API\n",
    "\n",
    "    def get_twitter_client_api(self):\n",
    "      return self.twitter_client\n",
    "\n",
    "    # this function is the one to pay attention to...Alter this using the tweepy documentation above\n",
    "    # here, we are applying the search method using tweepy's Cursor object to search for tweets\n",
    "    # our criteria is limited to a search query, geographic information and the number of tweets we'd like to request (see tweepy documentation)\n",
    "\n",
    "    def get_search_results(self, query, geo, num_tweets):\n",
    "        tweets = []\n",
    "        for tweet in Cursor(self.twitter_client.search, \n",
    "                            q = query, \n",
    "                            lang = 'en',\n",
    "                            geocode = geo, \n",
    "                            count = num_tweets).items(num_tweets):\n",
    "          tweets.append(tweet)\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VODzdZH6kUSu"
   },
   "outputs": [],
   "source": [
    "# tweet data comes in an efficient, but difficult-to-read format called JSON\n",
    "# this class allows us to run a basic sentiment analysis and create a legible pandas DataFrame for our tweets\n",
    "\n",
    "class TweetAnalyzer():\n",
    "\n",
    "  # these two functions set us up to run a basic sentiment analysis method from textblob\n",
    "  # we are cleaning the data by running a regular expression command and subsequently running a \n",
    "  # basic machine learning algorithm to estimate the polarity (pos/neg/neutral) of each tweet\n",
    "\n",
    "  def clean_tweet(self, tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "  def analyze_sentiment(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1 \n",
    "\n",
    "  # this function creates a pandas dataframe and uses list comprehension code to derive \n",
    "  # values from the list of tweets we generate. We can access other aspects of a tweet using\n",
    "  # different root-level attributes. See Tweet Object data dictionary link above\n",
    "\n",
    "  def tweets_to_data_frame(self, tweets):\n",
    "    df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['tweets'])\n",
    "\n",
    "    df['date'] = np.array([tweet.created_at.strftime(\"%m/%d/%Y %H:%M:%S\") for tweet in tweets])\n",
    "\n",
    "    df['date'] = df['date'].astype(object)\n",
    "\n",
    "    df['len'] = np.array([len(tweet.text) for tweet in tweets])\n",
    "    df['latlon'] = np.array([tweet.coordinates for tweet in tweets])\n",
    "    df['user_loc'] = np.array([tweet.user.location for tweet in tweets])\n",
    "    df['user_handle'] = np.array([tweet.user.screen_name for tweet in tweets])\n",
    "    df['followers'] = np.array([tweet.user.followers_count for tweet in tweets])\n",
    "    df['favorites'] = np.array([tweet.favorite_count for tweet in tweets])\n",
    "    df['retweets'] = np.array([tweet.retweet_count for tweet in tweets])\n",
    "\n",
    "    df = df.set_index(np.array([tweet.id for tweet in tweets]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMZtJR1jkb7x"
   },
   "outputs": [],
   "source": [
    "# don't worry too much about this if statement...this is us telling the python interpreter\n",
    "# to only run the following functions/statements if they appear in this script\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  # Question for you: what does it look like we are searching for here?\n",
    "\n",
    "  query = '\"prop 22\" -filter:retweets'\n",
    "  geo = '34.042201,-118.245854,100km'\n",
    "  num_tweets = 500\n",
    "  \n",
    "  twitter_client = TwitterClient()\n",
    "  tweet_analyzer = TweetAnalyzer()\n",
    "\n",
    "  tweets = twitter_client.get_search_results(query, geo, num_tweets)\n",
    "  df = tweet_analyzer.tweets_to_data_frame(tweets)\n",
    "\n",
    "  # Another question for you: what are we appending to our dataframe and why might these\n",
    "  # pieces of information be interesting to us?\n",
    "\n",
    "  df['tb_sentiment'] = np.array([tweet_analyzer.analyze_sentiment(tweet) for tweet in df['tweets']])\n",
    "  df['query_term'] = query\n",
    "  df['scrape_time'] = datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "  \n",
    "  df.index.name = 'id'\n",
    "\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24Bth8Otnxw6"
   },
   "source": [
    "**Sample search results:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TuS8Kill5L5"
   },
   "source": [
    "                                                                tweets  \\\n",
    "\n",
    "id                                                                       \n",
    "1313596177841975296  Human Rights Watch is against CA Prop 25 (2020...   \n",
    "1313535404172247041  Prop 25 is very contentious. I'm going to dig ...   \n",
    "1313382184024043521  @CoCoSouthLA @LAP\n",
    "aysAttention \"'with Propositi...   \n",
    "1313381072760041472  @CoCoSouthLA @LAPaysAttention Prop 25 essentia...   \n",
    "1313305141609549824  Very important read. I trust HRW on all things...   \n",
    " \n",
    "\n",
    "                                    date  len latlon                user_loc  \\\n",
    "id                                                                             \n",
    "1313596177841975296  10/06/2020 21:45:08   96   None         Los Angeles, CA   \n",
    "1313535404172247041  10/06/2020 17:43:39  133   None        Venice Beach, CA   \n",
    "1313382184024043521  10/06/2020 07:34:48  139   None         Los Angeles, CA   \n",
    "1313381072760041472  10/06/2020 07:30:23  140   None         Los Angeles, CA   \n",
    "1313305141609549824  10/06/2020 02:28:40   97   None             Los Angeles   \n",
    "   \n",
    "\n",
    "                         user_handle  followers  favorites  retweets  \\\n",
    "id                                                                     \n",
    "1313596177841975296  CalvinStarnesOG       4039          0         0   \n",
    "1313535404172247041      antifa_chad       1541          2         0   \n",
    "1313382184024043521      its_a_lotte        912          0         0   \n",
    "1313381072760041472      its_a_lotte        912          0         0   \n",
    "1313305141609549824     Benjaminlear        869          3         0   \n",
    "\n",
    "\n",
    "                     tb_sentiment                         query_term  \\\n",
    "id                                                                     \n",
    "1313596177841975296             0  \"proposition 25\" -filter:retweets   \n",
    "1313535404172247041             1  \"proposition 25\" -filter:retweets   \n",
    "1313382184024043521             0  \"proposition 25\" -filter:retweets   \n",
    "1313381072760041472             0  \"proposition 25\" -filter:retweets   \n",
    "1313305141609549824             1  \"proposition 25\" -filter:retweets   \n",
    "\n",
    "\n",
    "                             scrape_time  \n",
    "id                                        \n",
    "1313596177841975296  10/06/2020 17:44:53  \n",
    "1313535404172247041  10/06/2020 17:44:53  \n",
    "1313382184024043521  10/06/2020 17:44:53  \n",
    "1313381072760041472  10/06/2020 17:44:53  \n",
    "1313305141609549824  10/06/2020 17:44:53    >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHL_LcA3lHSG"
   },
   "source": [
    "### 1.2: Scraping Tweets and saving data to .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEhLw0ASlXc9"
   },
   "outputs": [],
   "source": [
    "# only run this line of code if you are creating a new file for your new search.\n",
    "# running this twice after creating a new csv will replace the information you assigned the first time\n",
    "\n",
    "#df.to_csv('../data/ca_prop_tweets.csv', index='True', index_label='id', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAunc4bAkxKo"
   },
   "outputs": [],
   "source": [
    "# here we are adding new search results to the .csv file we created. Thanks pandas!\n",
    "# we are dropping duplicate records in case our subsequent searches garner the same tweets\n",
    "\n",
    "def append_new_tweets(master_file, new_tweets):\n",
    "  master_file = pd.concat([master_file, new_tweets])\n",
    "  master_file = master_file.drop_duplicates(subset=['date', 'tweets'], keep='first')\n",
    "  master_file.to_csv('ca_prop_tweets.csv', \n",
    "                     index=True, \n",
    "                     index_label='id', \n",
    "                     encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zje3xhnYlFA5"
   },
   "outputs": [],
   "source": [
    "master_file = pd.read_csv('../data/ca_prop_tweets.csv', index_col='id')\n",
    "\n",
    "append_new_tweets(master_file, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keC-pQSer801"
   },
   "outputs": [],
   "source": [
    "# and voila! Let's take a look at our cleaned up .csv dataframe in the next section\n",
    "\n",
    "master_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTd_E2T4mzCy"
   },
   "source": [
    "## 2: Examining & manipulating unstructured text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAQfd7hholTi"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msOsJYaAld-Z"
   },
   "outputs": [],
   "source": [
    "# let's look at our gorgeous dataframe...\n",
    "# take 1 minute and see if anything strikes you...\n",
    "\n",
    "df = pd.read_csv('ca_prop_tweets.csv', index_col='id').sample(frac = 1)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1g2Y81Ix3uv"
   },
   "outputs": [],
   "source": [
    "# look familiar?\n",
    "\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cM0y79Ad3x-e"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-LIdXpqyFh2"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxlUc_ujqain"
   },
   "outputs": [],
   "source": [
    "df['tweets'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkWWib_23MJW"
   },
   "outputs": [],
   "source": [
    "# let's look at one more column and expand our visibility to see more of our tweets...\n",
    "# take two minutes and consider what the textblob sentiment analysis is capturing (1 = positive, -1 = negative, 0 = neutral)\n",
    "\n",
    "pd.options.display.max_colwidth = 300\n",
    "\n",
    "df[['tweets', 'tb_sentiment']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcR7ntpB46vV"
   },
   "source": [
    "One thing to consider: is my sentiment analysis providing meaningful information regarding my research question?\n",
    "\n",
    "<br>\n",
    "\n",
    "What kind of research questions can we address with this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNe2yiHoLvAM"
   },
   "outputs": [],
   "source": [
    "# How many occurances of no and yes are in our tweets?\n",
    "\n",
    "for t in df['tweets'][:10]:\n",
    "  print(t.lower().count(\"no\"), \"mention(s) of NO and\", t.lower().count(\"yes\"), \"mention(s) of YES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8ZFKL4-LpRJ"
   },
   "outputs": [],
   "source": [
    "# let's create two variables to explore trends in support/opposition\n",
    "# notice anything fishy? You might or you might not...\n",
    "\n",
    "df['num_yes'] = df['tweets'].str.lower().str.count(\"yes\")\n",
    "df['num_no'] = df['tweets'].str.lower().str.count('no')\n",
    "\n",
    "df[['num_yes', 'num_no', 'tweets']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfY20dEy7JDF"
   },
   "source": [
    "At this stage, we need to make a few assumptions about our data to begin to examine our RQ...those assumptions might be:\n",
    "\n",
    "\n",
    "*   records with more than 3 instances of NO and/or YES are likely voter guides\n",
    "*   records with 0 instances of NO AND YES will require further examination (that we do not currently have time for). We can drop these cases.\n",
    "*   records with equivalent NO and YES counts will also require further investigation and can be dropped for right now.\n",
    "*   records with a higher NO count than YES count likely signal opposition to the proposition, and vice versa (this is not always the case and further investigation is required before we can make this kind of assumption)\n",
    "*   Any others?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCeNvEV_Rt0o",
    "outputId": "9877e867-2ed9-487a-a996-1ebfcbc46003"
   },
   "outputs": [],
   "source": [
    "# Filtering out guides...\n",
    "\n",
    "mask_not_guide = (df['num_no'] < 4) & (df['num_yes'] <4) & ((df['num_no'] + df['num_yes']) <4)\n",
    "\n",
    "# Filtering out cases that do not state support or opposition with YES/NO\n",
    "\n",
    "mask_clear_position = (df['num_no'] > 0) | (df['num_yes'] > 0)\n",
    "\n",
    "# Filtering out cases with no clear position\n",
    "\n",
    "mask_unclear_position = (df['num_no'] == df['num_yes'])\n",
    "\n",
    "mask = mask_not_guide & mask_clear_position & ~mask_unclear_position\n",
    "\n",
    "df[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKN4LdcR-tMb"
   },
   "outputs": [],
   "source": [
    "# any better? \n",
    "\n",
    "df_new = df[mask]\n",
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddfxYzOWAh09"
   },
   "outputs": [],
   "source": [
    "# let's create a new variable, \"user_stance\", to ID supporters and opposers based on our criteria\n",
    "\n",
    "pd.set_option('mode.chained.assignment', None)\n",
    "\n",
    "mask_support = df_new['num_no'] < df_new['num_yes']\n",
    "\n",
    "df_new.loc[mask_support, 'user_stance'] = \"Support\"\n",
    "df_new.loc[~mask_support, 'user_stance'] = \"Oppose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFISATlIHMoK"
   },
   "outputs": [],
   "source": [
    "df_new[['tweets', 'user_stance']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhGh4SA0Hd_U"
   },
   "source": [
    "##3: Visualizing trends from content analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJ6yHr9QHrZ9"
   },
   "outputs": [],
   "source": [
    "# aaaaand one more piece of housekeeping: let's create a field to identify our propositions\n",
    "# pandas is loop averse...if anyone has any ideas for doing this operation more efficiently...please.\n",
    "\n",
    "mask_14 = df_new['query_term'].str.contains('14') == True\n",
    "mask_15 = df_new['query_term'].str.contains('15') == True\n",
    "mask_16 = df_new['query_term'].str.contains('16') == True\n",
    "mask_17 = df_new['query_term'].str.contains('17') == True\n",
    "mask_18 = df_new['query_term'].str.contains('18') == True\n",
    "mask_19 = df_new['query_term'].str.contains('19') == True\n",
    "mask_20 = df_new['query_term'].str.contains('20') == True\n",
    "mask_21 = df_new['query_term'].str.contains('21') == True\n",
    "mask_22 = df_new['query_term'].str.contains('22') == True\n",
    "mask_23 = df_new['query_term'].str.contains('23') == True\n",
    "mask_24 = df_new['query_term'].str.contains('24') == True\n",
    "mask_25 = df_new['query_term'].str.contains('25') == True\n",
    "\n",
    "df_new.loc[mask_14, 'proposition'] = \"Proposition 14\"\n",
    "df_new.loc[mask_15, 'proposition'] = \"Proposition 15\"\n",
    "df_new.loc[mask_16, 'proposition'] = \"Proposition 16\"\n",
    "df_new.loc[mask_17, 'proposition'] = \"Proposition 17\"\n",
    "df_new.loc[mask_18, 'proposition'] = \"Proposition 18\"\n",
    "df_new.loc[mask_19, 'proposition'] = \"Proposition 19\"\n",
    "df_new.loc[mask_20, 'proposition'] = \"Proposition 20\"\n",
    "df_new.loc[mask_21, 'proposition'] = \"Proposition 21\"\n",
    "df_new.loc[mask_22, 'proposition'] = \"Proposition 22\"\n",
    "df_new.loc[mask_23, 'proposition'] = \"Proposition 23\"\n",
    "df_new.loc[mask_24, 'proposition'] = \"Proposition 24\"\n",
    "df_new.loc[mask_25, 'proposition'] = \"Proposition 25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lb-b2W9NWWxZ"
   },
   "outputs": [],
   "source": [
    "# nice...\n",
    "\n",
    "df_new[['tweets', 'user_stance', 'proposition']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTyDkIq_WiTS"
   },
   "outputs": [],
   "source": [
    "# so how are Twitter users speaking about the CA props?\n",
    "\n",
    "df_new = df_new.sort_values(by=['proposition'])\n",
    "\n",
    "ax = sns.countplot(x=df_new['proposition'],\n",
    "                   hue=df_new['user_stance'],\n",
    "                   alpha=0.8)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), \n",
    "                   rotation=45, \n",
    "                   horizontalalignment='right')\n",
    "ax.set_xlabel('Nov 2020 California Propositions')\n",
    "ax.set_ylabel('Number of Unique Tweets')\n",
    "ax.set_title('Twitter User Stances on CA Props (October 2020)')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[2:], labels=labels[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5C0KsLtjUZd"
   },
   "outputs": [],
   "source": [
    "# Are influencers behaving any differently on twitter?\n",
    "\n",
    "mask_influencer = df_new['followers'] >=10000\n",
    "df_influencer = df_new[mask_influencer]\n",
    "\n",
    "ax = sns.countplot(x=df_influencer['proposition'],\n",
    "                   hue=df_influencer['user_stance'],\n",
    "                   alpha=0.8)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), \n",
    "                   rotation=45, \n",
    "                   horizontalalignment='right')\n",
    "ax.set_xlabel('Nov 2020 California Propositions')\n",
    "ax.set_ylabel('Number of Unique Tweets')\n",
    "ax.set_title('INFLUENCER Stances on CA Props (October 2020)')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[2:], labels=labels[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T53Wn7a4Jtl"
   },
   "source": [
    "# PART B: Basic Sentiment Analysis with Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFXKlDMonf0U"
   },
   "source": [
    "Now let's move on to another form of natural language processing using machine learning. In this section, we recreate from scratch a process similar to the textblob NLP method we applied to our Tweets earlier in this notebook. Together we will train an algorithm to identify positive and negative sentiment from ANY text using movie review data. In theory, you could use the algorithm we are training to study polarity in any text...but should you?\n",
    "\n",
    "<br>\n",
    "\n",
    "There are many different approaches to sentiment analysis we might take, but for today, let's explore a basic categorical approach that treats of texts as bags of words (BOW). A BOW approach to sentiment analysis focuses on sentimental value from the individual words in our texts, but ignores advanced information such as sarcasm or grammar. For simplicity's sake, we will be using a popular positive/negative sentiment lexicon developed by Hu & liu (2004) to compare with our movie reviews and to train our algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFmJdcM74VSu"
   },
   "source": [
    "## 4: Cleaning Text Data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGE2BGo84k3E"
   },
   "outputs": [],
   "source": [
    "# let's ask nltk to download a few important files to our current folder\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVzYuksN4tbj"
   },
   "outputs": [],
   "source": [
    "pos_folder = os.listdir('../data/train/pos')\n",
    "neg_folder = os.listdir('../data/train/neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdPriVZM42om"
   },
   "outputs": [],
   "source": [
    "print(len(pos_folder))\n",
    "print(len(neg_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2Sxgve-43UD"
   },
   "outputs": [],
   "source": [
    "print(pos_folder[0:5])\n",
    "print(neg_folder[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xusJzvzS45ts"
   },
   "outputs": [],
   "source": [
    "type(neg_folder[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQvIlCTO4760"
   },
   "outputs": [],
   "source": [
    "# You can open your .txt files in python to examine sentence structure, punctuation issues, etc.\n",
    "# These text files are pretty clean! We still have some grooming to do before we can use them to model...\n",
    "\n",
    "positive_review = open('../data/train/pos/'+pos_folder[0], 'r').read()\n",
    "negative_review = open('../data/train/neg/'+neg_folder[0], 'r').read()\n",
    "\n",
    "print('POSITIVE REVIEW:', positive_review)\n",
    "print('NEGATIVE REVIEW:', negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEdw8Bo75BDE"
   },
   "outputs": [],
   "source": [
    "# you can also shuffle through folders using the random package\n",
    "# you might use this method to acquaint yourself with large datasets like these\n",
    "\n",
    "random.shuffle(pos_folder)\n",
    "random.shuffle(neg_folder)\n",
    "\n",
    "positive_review = open('../data/train/pos/'+pos_folder[0], 'r').read()\n",
    "negative_review = open('../data/train/neg/'+neg_folder[0], 'r').read()\n",
    "\n",
    "print('RANDOM POSITIVE REVIEW:', positive_review)\n",
    "print('RANDOM NEGATIVE REVIEW:', negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44wA0CNL5Elz"
   },
   "outputs": [],
   "source": [
    "# let's take random samples from the negative and positive review folders to save processing time later on\n",
    "\n",
    "random.shuffle(pos_folder)\n",
    "pos_folder = pos_folder[:1500]\n",
    "\n",
    "random.shuffle(neg_folder)\n",
    "neg_folder = neg_folder[:1500]\n",
    "\n",
    "print(len(pos_folder))\n",
    "print(len(neg_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zBv8X4X5UP5"
   },
   "outputs": [],
   "source": [
    "# now let's create a list containing all of our opened movie review files...\n",
    "# by appending the .txt. file name to a path string\n",
    "\n",
    "files_positive = []\n",
    "files_negative = []\n",
    "\n",
    "for file in pos_folder:\n",
    "  files_positive.append(open('../data/train/pos/'+file, 'r').read())\n",
    "\n",
    "for file in neg_folder:\n",
    "  files_negative.append(open('../data/train/neg/'+file, 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmHcex835Vwi"
   },
   "outputs": [],
   "source": [
    "# aaaand here is a sample of our list\n",
    "\n",
    "print(files_positive[0])\n",
    "print(files_positive[1])\n",
    "print(files_positive[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRo51y3W5ZSU"
   },
   "outputs": [],
   "source": [
    "# removing all non-alphabetical content allows us to tokenize words if that is part of our NLP process.\n",
    "# here we are using something called a regular expression to clean our reviews...\n",
    "# this is an advanced topic, so do not stress the syntax\n",
    "\n",
    "no_punctuation = []\n",
    "\n",
    "for review in files_positive:\n",
    "  no_punctuation.append(re.sub(r'[^a-zA-Z\\s]','', review))\n",
    "\n",
    "no_punctuation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgAdMFsJ5fkI"
   },
   "outputs": [],
   "source": [
    "# and here we transformed all upper-case letters to lower-case letters\n",
    "\n",
    "pos_cleaned = []\n",
    "\n",
    "for string in no_punctuation:\n",
    "  pos_cleaned.append(string.lower())\n",
    "\n",
    "pos_cleaned[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEjSVtj95i--"
   },
   "outputs": [],
   "source": [
    "# ditto for our negative reviews\n",
    "\n",
    "neg_cleaned = []\n",
    "no_punct = []\n",
    "\n",
    "for review in files_negative:\n",
    "  neg_cleaned.append(re.sub(r'[^a-zA-Z\\s]','',review.lower()))\n",
    "\n",
    "neg_cleaned[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "san_RbTKI2-D"
   },
   "outputs": [],
   "source": [
    "all_cleaned = pos_cleaned + neg_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-0E3eFT5spu"
   },
   "source": [
    "## 5: Training, Validation, & Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QV05uwj6jNl"
   },
   "source": [
    "5.1: Cleaning, tokenizing, and creating a lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9ogH5Lt5rCU"
   },
   "outputs": [],
   "source": [
    "# Tokenizers are used to split strings into lists of substrings\n",
    "# word_tokenize() from the nltk package divides strings at punctuation marks other than periods.\n",
    "\n",
    "tokenized = []\n",
    "\n",
    "for review in pos_cleaned:\n",
    "    review_tokens = word_tokenize(review)\n",
    "    for word in review_tokens:\n",
    "      tokenized.append(word) \n",
    "\n",
    "print('There are', len(tokenized), 'words in my batch of positive reviews')\n",
    "print(tokenized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgsJqMoa6EkY"
   },
   "outputs": [],
   "source": [
    "# stop words are common 'empty' words that we can filter out to create space for words/phrases with sentimental weight\n",
    "# here we are using the stopwords constant that we downloaded earlier from NLTK, but you can create your own as well\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "sorted(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d03xGTKp6AYi"
   },
   "outputs": [],
   "source": [
    "no_stops = []\n",
    "\n",
    "for word in tokenized:\n",
    "    if word not in stop_words:\n",
    "      no_stops.append(word)\n",
    "\n",
    "print('I removed', (len(tokenized)-len(no_stops)), 'stop words!!')\n",
    "print(no_stops[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhqH3wGL6YDN"
   },
   "outputs": [],
   "source": [
    "# here we are asking nltk to tag all of our tokenized words with a semantic part of speech identifier\n",
    "\n",
    "part_of_speech_positive = nltk.pos_tag(no_stops)\n",
    "\n",
    "part_of_speech_positive[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWa6zyJT6b_5"
   },
   "outputs": [],
   "source": [
    "# allowed_word_types = [\"J\",\"R\",\"V\"]\n",
    "# J = adjectives, R = adverbs, and V = verbs\n",
    "\n",
    "adjectives = [\"J\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urCFO4_N6d63"
   },
   "outputs": [],
   "source": [
    "all_pos_adjectives = []\n",
    "\n",
    "for word in part_of_speech_positive:\n",
    "  if word[1][0] in adjectives:\n",
    "    all_pos_adjectives.append(word[0])\n",
    "\n",
    "all_pos_adjectives[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur6a6HtU6ffI"
   },
   "outputs": [],
   "source": [
    "all_neg_adjectives = []\n",
    "tokenizedb = []\n",
    "no_stopsb = []\n",
    "\n",
    "for review in neg_cleaned:\n",
    "    review_tokens = word_tokenize(review)\n",
    "    for word in review_tokens:\n",
    "      tokenizedb.append(word) \n",
    "\n",
    "\n",
    "for word in tokenizedb:\n",
    "    if word not in stop_words:\n",
    "      no_stopsb.append(word)\n",
    "\n",
    "part_of_speech_negative = nltk.pos_tag(no_stopsb)\n",
    "\n",
    "for word in part_of_speech_negative:\n",
    "  if word[1][0] in adjectives:\n",
    "    all_neg_adjectives.append(word[0])\n",
    "\n",
    "all_neg_adjectives[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6600ONcxgu3"
   },
   "outputs": [],
   "source": [
    "# and voila! \n",
    "\n",
    "lex = all_pos_adjectives + all_pos_adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACMQ1gd96sJf"
   },
   "source": [
    "4.2: Constructing your feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0zh5kLe2rPe"
   },
   "outputs": [],
   "source": [
    "# now onto the cool stuff...to begin our analysis, let's create a list of tuples with \n",
    "# our text information and pos/neg attributes on either side of each pair.\n",
    "\n",
    "documents = []\n",
    "\n",
    "for review in files_positive:\n",
    "  pos_docs = re.sub(r'[^a-zA-Z\\s]', '',review)\n",
    "  documents.append((pos_docs.lower(), \"pos\"))\n",
    "\n",
    "for review in files_negative:\n",
    "  neg_docs = re.sub(r'[^a-zA-Z\\s]', '',review)\n",
    "  documents.append((neg_docs.lower(), \"neg\"))\n",
    "\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_N6JaKTr61_s"
   },
   "outputs": [],
   "source": [
    "# in another module, you might use the lexicon we developed earlier, ID'ed the\n",
    "# most common words and used those strings to train our algorithm. \n",
    "# let's instead use a tried and true sentiment lexicon...now we are not limited to adjectives\n",
    "\n",
    "pos_file = open('../data/lexicon/pos.txt', 'r').read()\n",
    "neg_file = open('../data/lexicon/neg.txt', encoding = 'ISO=8859-1').read()\n",
    "\n",
    "pos_words = word_tokenize(pos_file)\n",
    "random.shuffle(pos_words)\n",
    "neg_words = word_tokenize(neg_file)\n",
    "random.shuffle(neg_words)\n",
    "\n",
    "all_sent_words = pos_words + neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANjduYNy31hj"
   },
   "outputs": [],
   "source": [
    "len(all_adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkRofZtd33za"
   },
   "outputs": [],
   "source": [
    "random.shuffle(all_sent_words)\n",
    "all_sent_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vHk0YX37iFA"
   },
   "outputs": [],
   "source": [
    "# this function creates a list of 'feature' dictionaries from each text file with\n",
    "# information about the presence of lexicon words in the file. \n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in all_sent_words:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dQjLVU17liP"
   },
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TbrJRt64gEg"
   },
   "outputs": [],
   "source": [
    "# if the above cell takes too long for you...use the following code to take a smaller random sample \n",
    "# of our sentiment lexicon. Any idea why it took so long? \n",
    "\n",
    "#random.shuffle(all_adjectives)\n",
    "#all_adjectives = all_adjectives[:5000]\n",
    "\n",
    "#featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRTDlIrS7nOs"
   },
   "outputs": [],
   "source": [
    "# this should explain why the prior cell took so long...\n",
    "\n",
    "featuresets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPxqw3Jv7o_I"
   },
   "outputs": [],
   "source": [
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tn35vpB87qMP"
   },
   "outputs": [],
   "source": [
    "# and now let's separate our data into training and test cases...\n",
    "\n",
    "training_set = featuresets[:2500]\n",
    "testing_set = featuresets[2500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMUHPFvs8DBw"
   },
   "source": [
    "## 6: Sentiment analysis with machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qFnh0OZ8UUi"
   },
   "outputs": [],
   "source": [
    "# So how did we do?\n",
    "# let's discuss what we've done here...\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Classifier accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "\n",
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdjHiTTn97sS"
   },
   "source": [
    "Our classifier works pretty well!! Although, your mileage may vary...Depending on the number of lexicon words you were able to incorporate, your accuracy percentage (the ratio of correctly predicted cases) should be around 80%.\n",
    "\n",
    "<br>\n",
    "\n",
    "There are many other classifying algorithms to try out, but we like this one for today. Let's figure out how to pack it away so we can test new data with it without having to retrain it over and over again. One way to do this is through a process called 'pickling'...yum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiuO-Wp19qu7"
   },
   "outputs": [],
   "source": [
    "# pickling allows us to store python objects like our classifier in byte format for simple recall\n",
    "# let's dump our classifier object into a pickle file\n",
    "\n",
    "# wb = write in bytes as opposed to strings...\n",
    "save_my_algorithm = open(\"../data/my_algorithm.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_my_algorithm)\n",
    "save_my_algorithm.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axvo1USUCywL"
   },
   "outputs": [],
   "source": [
    "# rb = read bytes in the pickle file\n",
    "my_algorithm = open(\"../data/naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(my_algorithm)\n",
    "my_algorithm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gXgU06WLeke"
   },
   "outputs": [],
   "source": [
    "# let's test our algorithm with individual text snippets to see how it performs...\n",
    "\n",
    "text = all_cleaned[0]\n",
    "feature = find_features(text)\n",
    "\n",
    "#and let's read the text to get a feel\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1X9NOqXoLs6j"
   },
   "outputs": [],
   "source": [
    "# last but not least, let's classify the individual feature with our pickled algorithm\n",
    "# how did it do for you?\n",
    "\n",
    "classifier.classify(feature)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3SKzZDa7i8aQ",
    "eHL_LcA3lHSG",
    "hTd_E2T4mzCy",
    "zhGh4SA0Hd_U",
    "UFmJdcM74VSu",
    "D-0E3eFT5spu"
   ],
   "name": "SENT_ANALYSIS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ppd534)",
   "language": "python",
   "name": "ox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
